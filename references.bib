@article{Somvanshi2025SurveyOK,
    author = {Somvanshi, Shriyank and Javed, Syed Aaqib and Islam, Md Monzurul and Pandit, Diwas and Das, Subasish},
    title = {A Survey on Kolmogorov-Arnold Network},
    year = {2025},
    issue_date = {January 2026},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {58},
    number = {2},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3743128},
    doi = {10.1145/3743128},
    abstract = {This review study explores the theoretical foundations, evolution, applications, and future potential of Kolmogorov–Arnold Networks (KAN), a neural network model inspired by the Kolmogorov–Arnold representation theorem. KANs set themselves apart from traditional neural networks by employing learnable, spline-parameterized functions rather than fixed activation functions, allowing for flexible and interpretable representations of high-dimensional functions. The review explores Kan’s architectural strengths, including adaptive edge-based activation functions that enhance parameter efficiency and scalability across varied applications such as time series forecasting, computational biomedicine, and graph learning. Key advancements including Temporal-KAN (T-KAN), FastKAN, and Partial Differential Equation (PDE) KAN illustrate KAN’s growing applicability in dynamic environments, significantly improving interpretability, computational efficiency, and adaptability for complex function approximation tasks. Moreover, the article discusses KANs integration with other architectures, such as convolutional, recurrent, and transformer-based models, showcasing its versatility in complementing established neural networks for tasks that require hybrid approaches. Despite its strengths, KAN faces computational challenges in high-dimensional and noisy data settings, sparking continued research into optimization strategies, regularization techniques, and hybrid models. This article highlights KANs expanding role in modern neural architectures and outlines future directions to enhance its computational efficiency, interpretability, and scalability in data-intensive applications.},
    journal = {ACM Comput. Surv.},
    month = sep,
    articleno = {55},
    numpages = {35},
    keywords = {Kolmogorov-arnold network}
}

@inproceedings{
  yang2025kolmogorovarnold,
  title={Kolmogorov-Arnold Transformer},
  author={Xingyi Yang, Xinchao Wang},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=BCeock53nt}
}
